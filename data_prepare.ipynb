{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import glob\n",
    "import pickle\n",
    "from util import load_data\n",
    "\n",
    "def K_fold_split(K=10, seed=0, fold=0):\n",
    "    test_ratio = 1/K\n",
    "    random.seed(seed)\n",
    "    #不同折seed必须一样\n",
    "    class_folders = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "    total_train_names = []\n",
    "    total_test_names = []\n",
    "    total_val_names = []\n",
    "    for i, folder in enumerate(class_folders):\n",
    "        name_list = glob.glob('data/original_data/type/%s/*.dat'%folder)\n",
    "        random.shuffle(name_list)\n",
    "        test_size = int(len(name_list) / K)\n",
    "        val_size = test_size\n",
    "        test_names = name_list[fold*test_size:(fold+1)*test_size]\n",
    "        val_and_train_names = list(set(name_list)-set(test_names))\n",
    "        val_names = random.sample(val_and_train_names, val_size)\n",
    "        train_names = list(set(val_and_train_names)-set(val_names))\n",
    "        total_train_names.append(train_names)\n",
    "        total_test_names.append(test_names)\n",
    "        total_val_names.append(val_names)\n",
    "    return total_train_names, total_val_names, total_test_names\n",
    "\n",
    "def load_original_data_K_fold(K=10, GP_limit=2500):\n",
    "    for i in range(0, K):\n",
    "        train_names, val_names, test_names = K_fold_split(K, fold=i)\n",
    "        print('loading data %dfold No%d'%(K,i))\n",
    "        train_data = [load_data(train_names[j], GP_model=True if len(train_names[j]) <= GP_limit else False) for j in range(0,11)]\n",
    "        #预先拟合好GP_model可以加快bagging所需的重采样过程（不用反复拟合）\n",
    "        #依然有重复计算，K折还是重复拟合了K倍，还可以优化一波，不过好像改起来比价麻烦，还是用计算机时间来节省我的时间吧\n",
    "        #修改计划有，在util里，有需要再继续吧\n",
    "        val_data = [load_data(val_names[j], GP_model=True) for j in range(0,11)]\n",
    "        test_data = [load_data(test_names[j], GP_model=False) for j in range(0,11)]\n",
    "        original_dataset = (train_data, val_data, test_data)\n",
    "        f = open('data/original_dataset_%dfold_No%d'%(K,i), 'wb')\n",
    "        pickle.dump(original_dataset, f)\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import create_dataset\n",
    "original_dataset = 'data/original_dataset_0.60_0.30_0.10'\n",
    "create_dataset(original_dataset, 3000, prefix='_uncertainty_image', down_sample=True, aug_val=True, image=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import create_dataset\n",
    "import os\n",
    "\n",
    "def create_dataset_K_fold(K=10):\n",
    "    for i in range(0, K):\n",
    "        original_dataset = 'data/original_dataset_%dfold_No%d'%(K,i)\n",
    "        for bag in range(0,10):\n",
    "            print('generating data fold_No%d bag No%d'%(i,bag))\n",
    "            create_dataset(original_dataset,2500, down_sample=True, instance=bag,\n",
    "                aug_val=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_original_data_K_fold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset_K_fold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(1,10):\n",
    "#     original_dataste = 'data/original_dataset_10fold_No%d'%i\n",
    "#     create_dataset(original_dataste, class_size=2500, down_sample=False)\n",
    "\n",
    "# original_dataste = 'data/original_dataset_10fold_No0'\n",
    "# create_dataset(original_dataste, class_size=15000, down_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import load_original_data\n",
    "load_original_data(test_ratio=0.3,val_ratio=0.1, seed=4325, GP_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import create_dataset \n",
    "\n",
    "for i in range(0,10):   \n",
    "    create_dataset('data/original_dataset_0.60_0.30_0.10', 1875, down_sample=True, aug_val=True,image=True, instance=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('DL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "686e52782356fa5192d8da796457d9836a4bb6aebab3dc056991bb89ef60f4a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
